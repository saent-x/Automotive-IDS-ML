{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "1. can-train-and-test dataset\n",
    "    - attack-free\n",
    "    - DoS\n",
    "    - accessory\n",
    "    - force-neutral\n",
    "    - rpm\n",
    "    - standstill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import datasets\n",
    "dataset_dir = './datasets/cantrainandtest/can-train-and-test/set_01/train_01/'\n",
    "\n",
    "attack_free_1 = pd.read_csv(dataset_dir + \"attack-free-1.csv\")\n",
    "attack_free_2 = pd.read_csv(dataset_dir + \"attack-free-2.csv\")\n",
    "\n",
    "DoS_1 = pd.read_csv(dataset_dir + \"DoS-1.csv\")\n",
    "DoS_2 = pd.read_csv(dataset_dir + \"DoS-2.csv\")\n",
    "\n",
    "accessory_1 = pd.read_csv(dataset_dir + \"accessory-1.csv\")\n",
    "accessory_2 = pd.read_csv(dataset_dir + \"accessory-2.csv\")\n",
    "\n",
    "force_neutral_1 = pd.read_csv(dataset_dir + \"force-neutral-1.csv\")\n",
    "force_neutral_2 = pd.read_csv(dataset_dir + \"force-neutral-2.csv\")\n",
    "\n",
    "rpm_1 = pd.read_csv(dataset_dir + \"rpm-1.csv\")\n",
    "rpm_2 = pd.read_csv(dataset_dir + \"rpm-2.csv\")\n",
    "\n",
    "standstill_1 = pd.read_csv(dataset_dir + \"standstill-1.csv\")\n",
    "standstill_2 = pd.read_csv(dataset_dir + \"standstill-2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## concatenate related datasets\n",
    "attack_free = pd.concat([attack_free_1, attack_free_2])\n",
    "DoS = pd.concat([DoS_1, DoS_2])\n",
    "accessory = pd.concat([accessory_1, accessory_2])\n",
    "force_neutral = pd.concat([force_neutral_1, force_neutral_2])\n",
    "rpm = pd.concat([rpm_1, rpm_2])\n",
    "standstill = pd.concat([standstill_1, standstill_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>arbitration_id</th>\n",
       "      <th>data_field</th>\n",
       "      <th>attack</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.672531e+09</td>\n",
       "      <td>0C1</td>\n",
       "      <td>3000000430000004</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.672531e+09</td>\n",
       "      <td>0C5</td>\n",
       "      <td>3000000430000004</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.672531e+09</td>\n",
       "      <td>184</td>\n",
       "      <td>000200000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.672531e+09</td>\n",
       "      <td>1C7</td>\n",
       "      <td>065CB9A200003F</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.672531e+09</td>\n",
       "      <td>1CD</td>\n",
       "      <td>0000000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1267139</th>\n",
       "      <td>1.672532e+09</td>\n",
       "      <td>199</td>\n",
       "      <td>CFFF0FFFEFFE00FF</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1267140</th>\n",
       "      <td>1.672532e+09</td>\n",
       "      <td>1A1</td>\n",
       "      <td>00000000000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1267141</th>\n",
       "      <td>1.672532e+09</td>\n",
       "      <td>1E5</td>\n",
       "      <td>46000E8000FFF201</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1267142</th>\n",
       "      <td>1.672532e+09</td>\n",
       "      <td>1C3</td>\n",
       "      <td>0696068F00000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1267143</th>\n",
       "      <td>1.672532e+09</td>\n",
       "      <td>2C3</td>\n",
       "      <td>085706A0071C</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10653140 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            timestamp arbitration_id        data_field  attack\n",
       "0        1.672531e+09            0C1  3000000430000004       0\n",
       "1        1.672531e+09            0C5  3000000430000004       0\n",
       "2        1.672531e+09            184      000200000000       0\n",
       "3        1.672531e+09            1C7    065CB9A200003F       0\n",
       "4        1.672531e+09            1CD        0000000000       0\n",
       "...               ...            ...               ...     ...\n",
       "1267139  1.672532e+09            199  CFFF0FFFEFFE00FF       0\n",
       "1267140  1.672532e+09            1A1    00000000000000       0\n",
       "1267141  1.672532e+09            1E5  46000E8000FFF201       0\n",
       "1267142  1.672532e+09            1C3  0696068F00000000       0\n",
       "1267143  1.672532e+09            2C3      085706A0071C       0\n",
       "\n",
       "[10653140 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## concatenate all data-samples into one\n",
    "can_train_and_test_ds = pd.concat([attack_free, DoS, accessory, force_neutral, rpm, standstill])\n",
    "can_train_and_test_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### change values of the attack in each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## since accessory mode is just another attack free mode we'd switch the \"0\" (attack-free) for \"1\" indicating accessory mode\n",
    "accessory['attack'] = accessory['attack'].replace(0,1)\n",
    "DoS['attack'] = DoS['attack'].replace(1,2)\n",
    "force_neutral['attack'] = force_neutral['attack'].replace(1,3)\n",
    "rpm['attack'] = rpm['attack'].replace(1,4)\n",
    "standstill['attack'] = standstill['attack'].replace(1,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### merge all subset into a single dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>arbitration_id</th>\n",
       "      <th>data_field</th>\n",
       "      <th>attack</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.672531e+09</td>\n",
       "      <td>0C1</td>\n",
       "      <td>3000000430000004</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.672531e+09</td>\n",
       "      <td>0C5</td>\n",
       "      <td>3000000430000004</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.672531e+09</td>\n",
       "      <td>184</td>\n",
       "      <td>000200000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.672531e+09</td>\n",
       "      <td>1C7</td>\n",
       "      <td>065CB9A200003F</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.672531e+09</td>\n",
       "      <td>1CD</td>\n",
       "      <td>0000000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1267139</th>\n",
       "      <td>1.672532e+09</td>\n",
       "      <td>199</td>\n",
       "      <td>CFFF0FFFEFFE00FF</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1267140</th>\n",
       "      <td>1.672532e+09</td>\n",
       "      <td>1A1</td>\n",
       "      <td>00000000000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1267141</th>\n",
       "      <td>1.672532e+09</td>\n",
       "      <td>1E5</td>\n",
       "      <td>46000E8000FFF201</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1267142</th>\n",
       "      <td>1.672532e+09</td>\n",
       "      <td>1C3</td>\n",
       "      <td>0696068F00000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1267143</th>\n",
       "      <td>1.672532e+09</td>\n",
       "      <td>2C3</td>\n",
       "      <td>085706A0071C</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10653140 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            timestamp arbitration_id        data_field  attack\n",
       "0        1.672531e+09            0C1  3000000430000004       0\n",
       "1        1.672531e+09            0C5  3000000430000004       0\n",
       "2        1.672531e+09            184      000200000000       0\n",
       "3        1.672531e+09            1C7    065CB9A200003F       0\n",
       "4        1.672531e+09            1CD        0000000000       0\n",
       "...               ...            ...               ...     ...\n",
       "1267139  1.672532e+09            199  CFFF0FFFEFFE00FF       0\n",
       "1267140  1.672532e+09            1A1    00000000000000       0\n",
       "1267141  1.672532e+09            1E5  46000E8000FFF201       0\n",
       "1267142  1.672532e+09            1C3  0696068F00000000       0\n",
       "1267143  1.672532e+09            2C3      085706A0071C       0\n",
       "\n",
       "[10653140 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_datasets = pd.concat([attack_free, accessory, DoS, force_neutral, rpm, standstill])\n",
    "\n",
    "filtered = merged_datasets[merged_datasets['attack'] == 1]\n",
    "\n",
    "merged_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One Hot Vector Encode the various attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deprecated\n",
    "# onv_ds = pd.get_dummies(merged_datasets, columns=['attack'], prefix='attack')\n",
    "\n",
    "# onv_ds['attack_0'] = onv_ds['attack_0'].astype(int)\n",
    "# onv_ds['attack_1'] = onv_ds['attack_1'].astype(int)\n",
    "# onv_ds['attack_2'] = onv_ds['attack_2'].astype(int)\n",
    "# onv_ds['attack_3'] = onv_ds['attack_3'].astype(int)\n",
    "# onv_ds['attack_4'] = onv_ds['attack_4'].astype(int)\n",
    "# onv_ds['attack_5'] = onv_ds['attack_5'].astype(int)\n",
    "\n",
    "# onv_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# onv_ds.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save new dataset file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# onv_ds.to_csv(\"updated_dataset.csv\", sep=',', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge and process test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import datasets\n",
    "dataset_dir = './datasets/cantrainandtest/can-train-and-test/set_01/test_01_known_vehicle_known_attack/'\n",
    "\n",
    "DoS_3 = pd.read_csv(dataset_dir + \"DoS-3.csv\")\n",
    "DoS_4 = pd.read_csv(dataset_dir + \"DoS-4.csv\")\n",
    "\n",
    "force_neutral_3 = pd.read_csv(dataset_dir + \"force-neutral-3.csv\")\n",
    "force_neutral_4 = pd.read_csv(dataset_dir + \"force-neutral-4.csv\")\n",
    "\n",
    "rpm_3 = pd.read_csv(dataset_dir + \"rpm-3.csv\")\n",
    "rpm_4 = pd.read_csv(dataset_dir + \"rpm-4.csv\")\n",
    "\n",
    "standstill_3 = pd.read_csv(dataset_dir + \"standstill-3.csv\")\n",
    "standstill_4 = pd.read_csv(dataset_dir + \"standstill-4.csv\")\n",
    "\n",
    "# merge related datasets\n",
    "DoS = pd.concat([DoS_3, DoS_4])\n",
    "force_neutral = pd.concat([force_neutral_3, force_neutral_4])\n",
    "rpm = pd.concat([rpm_3, rpm_4])\n",
    "standstill = pd.concat([standstill_3, standstill_4])\n",
    "\n",
    "# Label Encode Categorical Data\n",
    "label_encoder = preprocessing.LabelEncoder() \n",
    "\n",
    "DoS[\"arbitration_id\"] = label_encoder.fit_transform(DoS[\"arbitration_id\"])\n",
    "DoS[\"data_field\"] = label_encoder.fit_transform(DoS[\"data_field\"])\n",
    "\n",
    "force_neutral[\"arbitration_id\"] = label_encoder.fit_transform(force_neutral[\"arbitration_id\"])\n",
    "force_neutral[\"data_field\"] = label_encoder.fit_transform(force_neutral[\"data_field\"])\n",
    "\n",
    "rpm[\"arbitration_id\"] = label_encoder.fit_transform(rpm[\"arbitration_id\"])\n",
    "rpm[\"data_field\"] = label_encoder.fit_transform(rpm[\"data_field\"])\n",
    "\n",
    "standstill[\"arbitration_id\"] = label_encoder.fit_transform(standstill[\"arbitration_id\"])\n",
    "standstill[\"data_field\"] = label_encoder.fit_transform(standstill[\"data_field\"])\n",
    "\n",
    "# save to file\n",
    "DoS.to_csv(\"./datasets/clean-data/test-data/kv-ka/DoS.csv\", sep=',', index=False, encoding='utf-8')\n",
    "force_neutral.to_csv(\"./datasets/clean-data/test-data/kv-ka/force_neutral.csv\", sep=',', index=False, encoding='utf-8')\n",
    "rpm.to_csv(\"./datasets/clean-data/test-data/kv-ka/rpm.csv\", sep=',', index=False, encoding='utf-8')\n",
    "standstill.to_csv(\"./datasets/clean-data/test-data/kv-ka/standstill.csv\", sep=',', index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import datasets\n",
    "dataset_dir = './datasets/cantrainandtest/can-train-and-test/set_01/test_02_unknown_vehicle_known_attack/'\n",
    "\n",
    "DoS_3 = pd.read_csv(dataset_dir + \"DoS-3.csv\")\n",
    "DoS_4 = pd.read_csv(dataset_dir + \"DoS-4.csv\")\n",
    "\n",
    "force_neutral_3 = pd.read_csv(dataset_dir + \"force-neutral-3.csv\")\n",
    "force_neutral_4 = pd.read_csv(dataset_dir + \"force-neutral-4.csv\")\n",
    "\n",
    "rpm_3 = pd.read_csv(dataset_dir + \"rpm-3.csv\")\n",
    "rpm_4 = pd.read_csv(dataset_dir + \"rpm-4.csv\")\n",
    "\n",
    "standstill_3 = pd.read_csv(dataset_dir + \"standstill-3.csv\")\n",
    "standstill_4 = pd.read_csv(dataset_dir + \"standstill-4.csv\")\n",
    "\n",
    "# merge related datasets\n",
    "DoS = pd.concat([DoS_3, DoS_4])\n",
    "force_neutral = pd.concat([force_neutral_3, force_neutral_4])\n",
    "rpm = pd.concat([rpm_3, rpm_4])\n",
    "standstill = pd.concat([standstill_3, standstill_4])\n",
    "\n",
    "# Label Encode Categorical Data\n",
    "label_encoder = preprocessing.LabelEncoder() \n",
    "\n",
    "DoS[\"arbitration_id\"] = label_encoder.fit_transform(DoS[\"arbitration_id\"])\n",
    "DoS[\"data_field\"] = label_encoder.fit_transform(DoS[\"data_field\"])\n",
    "\n",
    "force_neutral[\"arbitration_id\"] = label_encoder.fit_transform(force_neutral[\"arbitration_id\"])\n",
    "force_neutral[\"data_field\"] = label_encoder.fit_transform(force_neutral[\"data_field\"])\n",
    "\n",
    "rpm[\"arbitration_id\"] = label_encoder.fit_transform(rpm[\"arbitration_id\"])\n",
    "rpm[\"data_field\"] = label_encoder.fit_transform(rpm[\"data_field\"])\n",
    "\n",
    "standstill[\"arbitration_id\"] = label_encoder.fit_transform(standstill[\"arbitration_id\"])\n",
    "standstill[\"data_field\"] = label_encoder.fit_transform(standstill[\"data_field\"])\n",
    "\n",
    "# save to file\n",
    "# DoS.to_csv(\"./datasets/clean-data/test-data/uv-ka/DoS.csv\", sep=',', index=False, encoding='utf-8')\n",
    "# force_neutral.to_csv(\"./datasets/clean-data/test-data/uv-ka/force_neutral.csv\", sep=',', index=False, encoding='utf-8')\n",
    "# rpm.to_csv(\"./datasets/clean-data/test-data/uv-ka/rpm.csv\", sep=',', index=False, encoding='utf-8')\n",
    "# standstill.to_csv(\"./datasets/clean-data/test-data/uv-ka/standstill.csv\", sep=',', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import datasets\n",
    "dataset_dir = './datasets/cantrainandtest/can-train-and-test/set_01/test_03_known_vehicle_unknown_attack/'\n",
    "\n",
    "double_3 = pd.read_csv(dataset_dir + \"double-3.csv\")\n",
    "double_4 = pd.read_csv(dataset_dir + \"double-4.csv\")\n",
    "fuzzing_3 = pd.read_csv(dataset_dir + \"fuzzing-3.csv\")\n",
    "fuzzing_4 = pd.read_csv(dataset_dir + \"fuzzing-4.csv\")\n",
    "interval_3 = pd.read_csv(dataset_dir + \"interval-3.csv\")\n",
    "interval_4 = pd.read_csv(dataset_dir + \"interval-4.csv\")\n",
    "speed_3 = pd.read_csv(dataset_dir + \"speed-3.csv\")\n",
    "speed_4 = pd.read_csv(dataset_dir + \"speed-4.csv\")\n",
    "systematic_3 = pd.read_csv(dataset_dir + \"systematic-3.csv\")\n",
    "systematic_4 = pd.read_csv(dataset_dir + \"systematic-4.csv\")\n",
    "triple_3 = pd.read_csv(dataset_dir + \"triple-3.csv\")\n",
    "triple_4 = pd.read_csv(dataset_dir + \"triple-4.csv\")\n",
    "\n",
    "# merge related datasets\n",
    "double = pd.concat([double_3, double_4])\n",
    "fuzzing = pd.concat([fuzzing_3, fuzzing_4])\n",
    "interval = pd.concat([interval_3, interval_4])\n",
    "speed = pd.concat([speed_3, speed_4])\n",
    "systematic = pd.concat([systematic_3, systematic_4])\n",
    "triple = pd.concat([triple_3, triple_4])\n",
    "\n",
    "# Label Encode Categorical Data\n",
    "label_encoder = preprocessing.LabelEncoder() \n",
    "\n",
    "double[\"arbitration_id\"] = label_encoder.fit_transform(double[\"arbitration_id\"])\n",
    "double[\"data_field\"] = label_encoder.fit_transform(double[\"data_field\"])\n",
    "\n",
    "fuzzing[\"arbitration_id\"] = label_encoder.fit_transform(fuzzing[\"arbitration_id\"])\n",
    "fuzzing[\"data_field\"] = label_encoder.fit_transform(fuzzing[\"data_field\"])\n",
    "\n",
    "interval[\"arbitration_id\"] = label_encoder.fit_transform(interval[\"arbitration_id\"])\n",
    "interval[\"data_field\"] = label_encoder.fit_transform(interval[\"data_field\"])\n",
    "\n",
    "speed[\"arbitration_id\"] = label_encoder.fit_transform(speed[\"arbitration_id\"])\n",
    "speed[\"data_field\"] = label_encoder.fit_transform(speed[\"data_field\"])\n",
    "\n",
    "systematic[\"arbitration_id\"] = label_encoder.fit_transform(systematic[\"arbitration_id\"])\n",
    "systematic[\"data_field\"] = label_encoder.fit_transform(systematic[\"data_field\"])\n",
    "\n",
    "triple[\"arbitration_id\"] = label_encoder.fit_transform(triple[\"arbitration_id\"])\n",
    "triple[\"data_field\"] = label_encoder.fit_transform(triple[\"data_field\"])\n",
    "\n",
    "# save to file\n",
    "# double.to_csv(\"./datasets/clean-data/test-data/kv-ua/double.csv\", sep=',', index=False, encoding='utf-8')\n",
    "# fuzzing.to_csv(\"./datasets/clean-data/test-data/kv-ua/fuzzing.csv\", sep=',', index=False, encoding='utf-8')\n",
    "# interval.to_csv(\"./datasets/clean-data/test-data/kv-ua/interval.csv\", sep=',', index=False, encoding='utf-8')\n",
    "# speed.to_csv(\"./datasets/clean-data/test-data/kv-ua/speed.csv\", sep=',', index=False, encoding='utf-8')\n",
    "# systematic.to_csv(\"./datasets/clean-data/test-data/kv-ua/systematic.csv\", sep=',', index=False, encoding='utf-8')\n",
    "# triple.to_csv(\"./datasets/clean-data/test-data/kv-ua/triple.csv\", sep=',', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import datasets\n",
    "dataset_dir = './datasets/cantrainandtest/can-train-and-test/set_01/test_04_unknown_vehicle_unknown_attack/'\n",
    "\n",
    "double_3 = pd.read_csv(dataset_dir + \"double-3.csv\")\n",
    "double_4 = pd.read_csv(dataset_dir + \"double-4.csv\")\n",
    "fuzzing_3 = pd.read_csv(dataset_dir + \"fuzzing-3.csv\")\n",
    "fuzzing_4 = pd.read_csv(dataset_dir + \"fuzzing-4.csv\")\n",
    "interval_3 = pd.read_csv(dataset_dir + \"interval-3.csv\")\n",
    "interval_4 = pd.read_csv(dataset_dir + \"interval-4.csv\")\n",
    "speed_3 = pd.read_csv(dataset_dir + \"speed-3.csv\")\n",
    "speed_4 = pd.read_csv(dataset_dir + \"speed-4.csv\")\n",
    "systematic_3 = pd.read_csv(dataset_dir + \"systematic-3.csv\")\n",
    "systematic_4 = pd.read_csv(dataset_dir + \"systematic-4.csv\")\n",
    "triple_3 = pd.read_csv(dataset_dir + \"triple-3.csv\")\n",
    "triple_4 = pd.read_csv(dataset_dir + \"triple-4.csv\")\n",
    "\n",
    "# merge related datasets\n",
    "double = pd.concat([double_3, double_4])\n",
    "fuzzing = pd.concat([fuzzing_3, fuzzing_4])\n",
    "interval = pd.concat([interval_3, interval_4])\n",
    "speed = pd.concat([speed_3, speed_4])\n",
    "systematic = pd.concat([systematic_3, systematic_4])\n",
    "triple = pd.concat([triple_3, triple_4])\n",
    "\n",
    "# Label Encode Categorical Data\n",
    "label_encoder = preprocessing.LabelEncoder() \n",
    "\n",
    "double[\"arbitration_id\"] = label_encoder.fit_transform(double[\"arbitration_id\"])\n",
    "double[\"data_field\"] = label_encoder.fit_transform(double[\"data_field\"])\n",
    "\n",
    "fuzzing[\"arbitration_id\"] = label_encoder.fit_transform(fuzzing[\"arbitration_id\"])\n",
    "fuzzing[\"data_field\"] = label_encoder.fit_transform(fuzzing[\"data_field\"])\n",
    "\n",
    "interval[\"arbitration_id\"] = label_encoder.fit_transform(interval[\"arbitration_id\"])\n",
    "interval[\"data_field\"] = label_encoder.fit_transform(interval[\"data_field\"])\n",
    "\n",
    "speed[\"arbitration_id\"] = label_encoder.fit_transform(speed[\"arbitration_id\"])\n",
    "speed[\"data_field\"] = label_encoder.fit_transform(speed[\"data_field\"])\n",
    "\n",
    "systematic[\"arbitration_id\"] = label_encoder.fit_transform(systematic[\"arbitration_id\"])\n",
    "systematic[\"data_field\"] = label_encoder.fit_transform(systematic[\"data_field\"])\n",
    "\n",
    "triple[\"arbitration_id\"] = label_encoder.fit_transform(triple[\"arbitration_id\"])\n",
    "triple[\"data_field\"] = label_encoder.fit_transform(triple[\"data_field\"])\n",
    "\n",
    "# save to file\n",
    "# double.to_csv(\"./datasets/clean-data/test-data/uv-ua/double.csv\", sep=',', index=False, encoding='utf-8')\n",
    "# fuzzing.to_csv(\"./datasets/clean-data/test-data/uv-ua/fuzzing.csv\", sep=',', index=False, encoding='utf-8')\n",
    "# interval.to_csv(\"./datasets/clean-data/test-data/uv-ua/interval.csv\", sep=',', index=False, encoding='utf-8')\n",
    "# speed.to_csv(\"./datasets/clean-data/test-data/uv-ua/speed.csv\", sep=',', index=False, encoding='utf-8')\n",
    "# systematic.to_csv(\"./datasets/clean-data/test-data/uv-ua/systematic.csv\", sep=',', index=False, encoding='utf-8')\n",
    "# triple.to_csv(\"./datasets/clean-data/test-data/uv-ua/triple.csv\", sep=',', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Encode columns with categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Label Encode Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = preprocessing.LabelEncoder() \n",
    "# Update categorical\n",
    "merged_datasets[\"arbitration_id\"] = label_encoder.fit_transform(merged_datasets[\"arbitration_id\"])\n",
    "merged_datasets[\"data_field\"] = label_encoder.fit_transform(merged_datasets[\"data_field\"])\n",
    "\n",
    "merged_datasets[:10]\n",
    "\n",
    "# save new training dataset\n",
    "merged_datasets.to_csv(\"updated_dataset.csv\", sep=',', index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DoS_df = pd.read_csv(\"./datasets/clean-data/test-data/kv-ka/DoS.csv\")\n",
    "\n",
    "new_DoS_df = DoS_df.drop(columns=[\"attack\"])\n",
    "\n",
    "# new_DoS_df\n",
    "\n",
    "new_DoS_df.to_csv(\"./datasets/clean-data/test-data/kv-ka/new_DoS.csv\", sep=',', index=False, encoding='utf-8')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
